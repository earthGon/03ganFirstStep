{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MnistDataset(Dataset):\n",
    "    data = pd.read_csv(\"mnist_train.csv\", header=None)\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.array = MnistDataset.data.to_numpy()\n",
    "\n",
    "        # self.dataset -> numpy\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.array)\n",
    "\n",
    "    def __getitem__(self, index) :\n",
    "        target = self.array[index,0]\n",
    "        boolean_indexing = np.zeros(10)\n",
    "        boolean_indexing[target] = 1.0\n",
    "        return torch.FloatTensor(self.array[index,1:].reshape(28,28)/255.0),torch.FloatTensor(boolean_indexing)\n",
    "    def plot_img(self,index):\n",
    "        data = self.array[index,1:].reshape(28,28)\n",
    "        plt.figure(figsize=(3,3))\n",
    "        plt.imshow(data,cmap='Blues')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnistdataset = MnistDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.4863, 0.9922, 1.0000, 0.2471, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3765,\n",
      "         0.9569, 0.9843, 0.9922, 0.2431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4980,\n",
      "         0.9843, 0.9843, 0.9922, 0.2431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2667, 0.9255,\n",
      "         0.9843, 0.8275, 0.1216, 0.0314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2353, 0.8941, 0.9843,\n",
      "         0.9843, 0.3686, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9922, 0.9922,\n",
      "         0.7412, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0784, 0.9922, 0.9843, 0.9216,\n",
      "         0.2588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1255, 0.8039, 0.9922, 0.9843, 0.4941,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.4078, 0.9843, 0.9922, 0.7216, 0.0588,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.3137, 0.9412, 0.9843, 0.7569, 0.0902, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.1255, 0.9922, 0.9922, 0.9922, 0.6235, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.5922, 0.9843, 0.9843, 0.9843, 0.1529, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1882, 0.8667, 0.9843, 0.9843, 0.6745, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.9176, 0.9843, 0.9843, 0.7686, 0.0471, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.9922, 0.9843, 0.9843, 0.3490, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.6235, 1.0000, 0.9922, 0.9922, 0.1216, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1882,\n",
      "         0.8941, 0.9922, 0.9686, 0.5490, 0.0314, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2510,\n",
      "         0.9843, 0.9922, 0.8627, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2510,\n",
      "         0.9843, 0.9922, 0.8627, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0941,\n",
      "         0.7569, 0.9922, 0.8627, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]]), tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARYAAAEUCAYAAADuhRlEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAT+klEQVR4nO3dcWzUZZ7H8U+LdCjYTi2lM+3R1oJuNYvgHkqtIAHpUbp3nAi5iLubgCGy607JQd2oTQQW12QWvChRK70/DNXsAi4JhYNc6mKBcubabqiwXLPSBUKkXjt15bYzpUDp0ef+8BgzUqe084wzA+9X8iTM73nm93z92fnkmd/85jdJxhgjALAoOdYFALj1ECwArCNYAFhHsACwjmABYB3BAsA6ggWAdQQLAOsIFgDW3RHrAr5pcHBQnZ2dSktLU1JSUqzLAfD/jDHq7e1Vbm6ukpOHWZOYKHn77bdNQUGBcTgcZtasWaalpeWmntfR0WEk0Wi0OG0dHR3Dvo6jsmL54IMPVFlZqZqaGhUXF2vr1q0qKytTe3u7srOzwz43LS1NknTmXIfS0tOjUR6AUegNBHRPYV7wNRpOkjH2v4RYXFyshx9+WG+//bakr97e5OXlac2aNXrppZfCPjcQCMjpdKr7gl/pBAsQNwKBgFwTnfL7h39tWj95e/XqVbW2tqq0tPTrSZKTVVpaqqamphvG9/f3KxAIhDQAic16sHz55Ze6du2aXC5XyHaXyyWfz3fDeK/XK6fTGWx5eXm2SwLwHYv5x81VVVXy+/3B1tHREeuSAETI+snbrKwsjRkzRt3d3SHbu7u75Xa7bxjvcDjkcDhslwEghqyvWFJSUjRz5kw1NDQEtw0ODqqhoUElJSW2pwMQh6LycXNlZaVWrFihhx56SLNmzdLWrVvV19enZ555JhrTAYgzUQmWp556Sn/5y1+0YcMG+Xw+Pfjgg6qvr7/hhC6AW1NUrmOJBNexAPEpptexAADBAsA6ggWAdQQLAOsIFgDWESwArCNYAFhHsACwjmABYB3BAsA6ggWAdQQLAOsIFgDWESwArCNYAFhHsACwjmABYB3BAsA6ggWAdQQLAOsIFgDWESwArCNYAFhHsACwjmABYB3BAsA6ggWAdQQLAOsIFgDWESwArLvD9g5/+ctfatOmTSHbioqKdOrUKdtTATel6t+H/9ur2bQt/IDBa2G7D+1+NWz/D+7OGLaGW4n1YJGk73//+/roo4++nuSOqEwDIE5F5RV/xx13yO12R2PXABJAVM6xnD59Wrm5uZoyZYp+/OMf6/z589GYBkCcsr5iKS4uVm1trYqKitTV1aVNmzbpscceU1tbm9LS0m4Y39/fr/7+/uDjQCBguyQA3zHrwVJeXh789/Tp01VcXKyCggL97ne/06pVq24Y7/V6bzjZCyCxRf3j5oyMDH3ve9/TmTNnhuyvqqqS3+8Pto6OjmiXBCDKoh4sFy9e1NmzZ5WTkzNkv8PhUHp6ekgDkNisvxX6xS9+ocWLF6ugoECdnZ3auHGjxowZo6efftr2VIAkaccnn4Xtr9nym+F3kjwmohqSInr2rcd6sHz++ed6+umndeHCBU2aNElz5sxRc3OzJk2aZHsqAHHKerDs2rXL9i4BJBi+KwTAOoIFgHUECwDrCBYA1hEsAKwjWABYx41SkPA++e9L4Qf0D9MP61ixALCOYAFgHcECwDqCBYB1BAsA6wgWANYRLACs4zoWxL2Ws/8Ttv/d6n+LeI7U+x8K29/6+tKw/ZkTxkZcw62EFQsA6wgWANYRLACsI1gAWEewALCOYAFgHcECwDquY0HM/fGznrD9i57fGX4H/u6Ia3hjzZyw/TkZ4yKe43bCigWAdQQLAOsIFgDWESwArCNYAFhHsACwjmABYB3XsSDmNn7YHn5A158j2v/fzF807JinfpAf0RwINeIVy9GjR7V48WLl5uYqKSlJe/fuDek3xmjDhg3KyclRamqqSktLdfr0aVv1AkgAIw6Wvr4+zZgxQ9XV1UP2b9myRW+++aZqamrU0tKiCRMmqKysTFeuXIm4WACJYcRvhcrLy1VeXj5knzFGW7du1csvv6wnnnhCkvT+++/L5XJp7969Wr58eWTVAkgIVk/enjt3Tj6fT6WlpcFtTqdTxcXFampqGvI5/f39CgQCIQ1AYrMaLD6fT5LkcrlCtrtcrmDfN3m9XjmdzmDLy8uzWRKAGIj5x81VVVXy+/3B1tHREeuSAETIarC43W5JUnd36NfYu7u7g33f5HA4lJ6eHtIAJDarwVJYWCi3262GhobgtkAgoJaWFpWUlNicCkAcG/GnQhcvXtSZM2eCj8+dO6cTJ04oMzNT+fn5Wrt2rV599VXde++9Kiws1Pr165Wbm6slS5bYrBsJoqfv6rBjGt/dEX5A8pjw/RlDr4avq139yLA1wK4RB8uxY8c0f/784OPKykpJ0ooVK1RbW6sXXnhBfX19Wr16tXp6ejRnzhzV19dr3DjuwAXcLkYcLPPmzZMx5lv7k5KS9Morr+iVV16JqDAAiSvmnwoBuPUQLACsI1gAWEewALCOYAFgHTd6QkQ6/3o5bH9J1YGo1/CsZ3HY/oem3BX1GhCKFQsA6wgWANYRLACsI1gAWEewALCOYAFgHcECwDquY0FE9p8a+l7G1wX+OPRN1Efi7tKysP0bSu+NeA7YxYoFgHUECwDrCBYA1hEsAKwjWABYR7AAsI5gAWAd17EgrN9/Gv46lZfW/ybiOVyP/V3Y/o9enB+2/85x/BnHG1YsAKwjWABYR7AAsI5gAWAdwQLAOoIFgHUECwDrCBYA1o34yqKjR4/qtddeU2trq7q6ulRXV6clS5YE+1euXKn33nsv5DllZWWqr6+PuFjYN9wPjj210hv1GqZOnRi2f+KdKVGvAXaNeMXS19enGTNmqLq6+lvHLFq0SF1dXcG2c+fOiIoEkFhGvGIpLy9XeXl52DEOh0Nut3vURQFIbFE5x3LkyBFlZ2erqKhIzz33nC5cuPCtY/v7+xUIBEIagMRmPVgWLVqk999/Xw0NDdq8ebMaGxtVXl6ua9euDTne6/XK6XQGW15enu2SAHzHrH8tdPny5cF/P/DAA5o+fbqmTp2qI0eOaMGCBTeMr6qqUmVlZfBxIBAgXIAEF/WPm6dMmaKsrCydOXNmyH6Hw6H09PSQBiCxRT1YPv/8c124cEE5OTnRngpAnBjxW6GLFy+GrD7OnTunEydOKDMzU5mZmdq0aZOWLVsmt9uts2fP6oUXXtA999yjsrLwPzqF2PjnurbwA5LHRL2Gd/5pRtTnwHdrxMFy7NgxzZ//9R29rp8fWbFihbZt26aTJ0/qvffeU09Pj3Jzc7Vw4UL96le/ksPhsFc1gLg24mCZN2+ejDHf2v/hhx9GVBCAxMd3hQBYR7AAsI5gAWAdwQLAOoIFgHX80tMt7M9dvcOOafj9f0W1hmlL/nHYMQVZ46NaA757rFgAWEewALCOYAFgHcECwDqCBYB1BAsA6wgWANZxHcstrHjVvw4/6K+dEc0x6ZF5Yfs/XDsnov0jMbFiAWAdwQLAOoIFgHUECwDrCBYA1hEsAKwjWABYR7AAsI4L5G5lFzqGHxPhD5JtfuZvw/aPd/AndjtixQLAOoIFgHUECwDrCBYA1hEsAKwjWABYR7AAsG5EFxl4vV7t2bNHp06dUmpqqh599FFt3rxZRUVFwTFXrlzR888/r127dqm/v19lZWV655135HK5rBd/u/v7bU3hBwxei3oNj+RnRX0OJJ4RrVgaGxvl8XjU3NysgwcPamBgQAsXLlRfX19wzLp167R//37t3r1bjY2N6uzs1NKlS60XDiB+jWjFUl9fH/K4trZW2dnZam1t1dy5c+X3+/Xuu+9qx44devzxxyVJ27dv1/3336/m5mY98sgj9ioHELciOsfi9/slSZmZmZKk1tZWDQwMqLS0NDjmvvvuU35+vpqahl629/f3KxAIhDQAiW3UwTI4OKi1a9dq9uzZmjZtmiTJ5/MpJSVFGRkZIWNdLpd8Pt+Q+/F6vXI6ncGWl5c32pIAxIlRB4vH41FbW5t27doVUQFVVVXy+/3B1tFxE1+cAxDXRvXV04qKCh04cEBHjx7V5MmTg9vdbreuXr2qnp6ekFVLd3e33G73kPtyOBxyOByjKQNAnBrRisUYo4qKCtXV1enQoUMqLCwM6Z85c6bGjh2rhoaG4Lb29nadP39eJSUldioGEPdGtGLxeDzasWOH9u3bp7S0tOB5E6fTqdTUVDmdTq1atUqVlZXKzMxUenq61qxZo5KSEj4RGoU/d/WG7f/PDz8Jv4ObuddKSmrY7qWe5WH7MyeMHX4O3HZGFCzbtm2TJM2bNy9k+/bt27Vy5UpJ0htvvKHk5GQtW7Ys5AI5ALePEQWLMWbYMePGjVN1dbWqq6tHXRSAxMZ3hQBYR7AAsI5gAWAdwQLAOoIFgHX86Escu3jlf8MP6D4b8RzJk4vC9r+7/MGI58DthxULAOsIFgDWESwArCNYAFhHsACwjmABYB3BAsA6ggWAdQQLAOsIFgDWESwArCNYAFhHsACwjmABYB3BAsA6ggWAddzoKY4VZI0P2++a/XjY/u7/OGizHOCmsWIBYB3BAsA6ggWAdQQLAOsIFgDWESwArCNYAFg3outYvF6v9uzZo1OnTik1NVWPPvqoNm/erKKir3/0at68eWpsbAx53k9/+lPV1NTYqfg2MjHNEbb/1L/8wzB7GK4fiI4RrVgaGxvl8XjU3NysgwcPamBgQAsXLlRfX1/IuGeffVZdXV3BtmXLFqtFA4hvI1qx1NfXhzyura1Vdna2WltbNXfu3OD28ePHy+1226kQQMKJ6ByL3++XJGVmZoZs/+1vf6usrCxNmzZNVVVVunTpUiTTAEgwo/6u0ODgoNauXavZs2dr2rRpwe0/+tGPVFBQoNzcXJ08eVIvvvii2tvbtWfPniH309/fr/7+/uDjQCAw2pIAxIlRB4vH41FbW5s+/vjjkO2rV68O/vuBBx5QTk6OFixYoLNnz2rq1Kk37Mfr9WrTpk2jLQNAHBrVW6GKigodOHBAhw8f1uTJk8OOLS4uliSdOXNmyP6qqir5/f5g6+joGE1JAOLIiFYsxhitWbNGdXV1OnLkiAoLC4d9zokTJyRJOTk5Q/Y7HA45HOE/VgWQWEYULB6PRzt27NC+ffuUlpYmn88nSXI6nUpNTdXZs2e1Y8cO/fCHP9TEiRN18uRJrVu3TnPnztX06dOj8h8AIP4kGWPMTQ9OShpy+/bt27Vy5Up1dHToJz/5idra2tTX16e8vDw9+eSTevnll5Wenn5TcwQCATmdTnVf8N/0cwBEXyAQkGuiU37/8K/NEb8VCicvL++Gq24B3H74rhAA6wgWANYRLACsI1gAWEewALCOYAFgHcECwDqCBYB1BAsA6wgWANYRLACsI1gAWEewALBu1LemjJbr36Du5d63QFy5/pq8mTutxF2w9Pb2SpLuKcyLcSUAhtLb2yun0xl2zIhu9PRdGBwcVGdnp9LS0pSUlKRAIKC8vDx1dHRw46cIcSztuF2PozFGvb29ys3NVXJy+LMocbdiSU5OHvIG3enp6bfV/8Ro4ljacTsex+FWKtdx8haAdQQLAOviPlgcDoc2btzIT4RYwLG0g+M4vLg7eQsg8cX9igVA4iFYAFhHsACwjmABYF3cB0t1dbXuvvtujRs3TsXFxfrDH/4Q65Li3tGjR7V48WLl5uYqKSlJe/fuDek3xmjDhg3KyclRamqqSktLdfr06dgUG8e8Xq8efvhhpaWlKTs7W0uWLFF7e3vImCtXrsjj8WjixIm68847tWzZMnV3d8eo4vgR18HywQcfqLKyUhs3btQnn3yiGTNmqKysTF988UWsS4trfX19mjFjhqqrq4fs37Jli958803V1NSopaVFEyZMUFlZma5cufIdVxrfGhsb5fF41NzcrIMHD2pgYEALFy5UX19fcMy6deu0f/9+7d69W42Njers7NTSpUtjWHWcMHFs1qxZxuPxBB9fu3bN5ObmGq/XG8OqEoskU1dXF3w8ODho3G63ee2114Lbenp6jMPhMDt37oxBhYnjiy++MJJMY2OjMear4zZ27Fize/fu4JhPP/3USDJNTU2xKjMuxO2K5erVq2ptbVVpaWlwW3JyskpLS9XU1BTDyhLbuXPn5PP5Qo6r0+lUcXExx3UYfr9fkpSZmSlJam1t1cDAQMixvO+++5Sfn3/bH8u4DZYvv/xS165dk8vlCtnucrnk8/liVFXiu37sOK4jMzg4qLVr12r27NmaNm2apK+OZUpKijIyMkLGcizj8NvNQDzyeDxqa2vTxx9/HOtSEkLcrliysrI0ZsyYG86wd3d3y+12x6iqxHf92HFcb15FRYUOHDigw4cPh9zSw+126+rVq+rp6QkZz7GM42BJSUnRzJkz1dDQENw2ODiohoYGlZSUxLCyxFZYWCi32x1yXAOBgFpaWjiu32CMUUVFherq6nTo0CEVFhaG9M+cOVNjx44NOZbt7e06f/48xzLWZ4/D2bVrl3E4HKa2ttb86U9/MqtXrzYZGRnG5/PFurS41tvba44fP26OHz9uJJnXX3/dHD9+3Hz22WfGGGN+/etfm4yMDLNv3z5z8uRJ88QTT5jCwkJz+fLlGFceX5577jnjdDrNkSNHTFdXV7BdunQpOOZnP/uZyc/PN4cOHTLHjh0zJSUlpqSkJIZVx4e4DhZjjHnrrbdMfn6+SUlJMbNmzTLNzc2xLinuHT582Ei6oa1YscIY89VHzuvXrzcul8s4HA6zYMEC097eHtui49BQx1CS2b59e3DM5cuXzc9//nNz1113mfHjx5snn3zSdHV1xa7oOMFtEwBYF7fnWAAkLoIFgHUECwDrCBYA1hEsAKwjWABYR7AAsI5gAWAdwQLAOoIFgHUECwDrCBYA1v0fm7UNdAeEXWEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(mnistdataset[3])\n",
    "mnistdataset.plot_img(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myView(torch.nn.Module):\n",
    "    def __init__(self,shape):\n",
    "        super().__init__()\n",
    "        self.shape = shape,\n",
    "    def forward(self,x):\n",
    "        return x.view(*self.shape)\n",
    "    #이게 되려면, x는 뭐지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "\n",
    "# model -> conv2d\n",
    "# optim\n",
    "# loss\n",
    "# train\n",
    "# loss hist\n",
    "from torch.nn import BCELoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "class D(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1,10,5,2),\n",
    "            torch.nn.LeakyReLU(0.02),\n",
    "            torch.nn.BatchNorm2d(10),\n",
    "            \n",
    "            torch.nn.Conv2d(10,10,3,2),\n",
    "            torch.nn.LeakyReLU(0.02),\n",
    "            torch.nn.BatchNorm2d(10),\n",
    "\n",
    "            myView((250)),\n",
    "            torch.nn.Linear(250,10),            \n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        self.optimizer = Adam(self.parameters(),lr=0.0002)\n",
    "        self.loss_fn = BCELoss()\n",
    "        self.loss_hist = []\n",
    "        self.counter=0\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "    def train(self,x,target): #(1,1,28,28)로 받음 ->rgb면 transpose해주고 (batchsize,3,h,w)로 해주겠네?\n",
    "        self.counter+=1\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        y_pred = self.model(x)\n",
    "        loss = self.loss_fn(y_pred,target) # target은 boolean indexing 가능하게\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.loss_hist.append(loss.item())\n",
    "        if self.counter %100 ==0:\n",
    "            print(f'iter {self.counter}, loss: {self.loss_hist[-1]}')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 100, loss: 0.37882906198501587\n",
      "iter 200, loss: 0.4101315438747406\n",
      "iter 300, loss: 0.3353221118450165\n",
      "iter 400, loss: 0.29799288511276245\n",
      "iter 500, loss: 0.2552087903022766\n",
      "iter 600, loss: 0.22562828660011292\n",
      "iter 700, loss: 0.29445356130599976\n",
      "iter 800, loss: 0.23623552918434143\n",
      "iter 900, loss: 0.36321061849594116\n",
      "iter 1000, loss: 0.24973657727241516\n",
      "iter 1100, loss: 0.14085806906223297\n",
      "iter 1200, loss: 0.22672784328460693\n",
      "iter 1300, loss: 0.18900689482688904\n",
      "iter 1400, loss: 0.08725627511739731\n",
      "iter 1500, loss: 0.0816589966416359\n",
      "iter 1600, loss: 0.11779574304819107\n",
      "iter 1700, loss: 0.061874061822891235\n",
      "iter 1800, loss: 0.10278959572315216\n",
      "iter 1900, loss: 0.061141688376665115\n",
      "iter 2000, loss: 0.09836773574352264\n",
      "iter 2100, loss: 0.09903436154127121\n",
      "iter 2200, loss: 0.10290338099002838\n",
      "iter 2300, loss: 0.14349444210529327\n",
      "iter 2400, loss: 0.056196559220552444\n",
      "iter 2500, loss: 0.06639297306537628\n",
      "iter 2600, loss: 0.054381974041461945\n",
      "iter 2700, loss: 0.13586491346359253\n",
      "iter 2800, loss: 0.10017092525959015\n",
      "iter 2900, loss: 0.024613287299871445\n",
      "iter 3000, loss: 0.06050143390893936\n",
      "iter 3100, loss: 0.061547208577394485\n",
      "iter 3200, loss: 0.051919125020504\n",
      "iter 3300, loss: 0.052026353776454926\n",
      "iter 3400, loss: 0.07841650396585464\n",
      "iter 3500, loss: 0.03987615928053856\n",
      "iter 3600, loss: 0.08976925909519196\n",
      "iter 3700, loss: 0.04441194608807564\n",
      "iter 3800, loss: 0.06556127965450287\n",
      "iter 3900, loss: 0.01601431891322136\n",
      "iter 4000, loss: 0.03151438385248184\n",
      "iter 4100, loss: 0.03821662440896034\n",
      "iter 4200, loss: 0.016024410724639893\n",
      "iter 4300, loss: 0.03624714910984039\n",
      "iter 4400, loss: 0.06430565565824509\n",
      "iter 4500, loss: 0.01911288872361183\n",
      "iter 4600, loss: 0.012066589668393135\n",
      "iter 4700, loss: 0.051542628556489944\n",
      "iter 4800, loss: 0.12563416361808777\n",
      "iter 4900, loss: 0.02597966231405735\n",
      "iter 5000, loss: 0.07031318545341492\n",
      "iter 5100, loss: 0.011527004651725292\n",
      "iter 5200, loss: 0.027024786919355392\n",
      "iter 5300, loss: 0.06750912964344025\n",
      "iter 5400, loss: 0.03478371351957321\n",
      "iter 5500, loss: 0.06688705831766129\n",
      "iter 5600, loss: 0.017398769035935402\n",
      "iter 5700, loss: 0.05993528291583061\n",
      "iter 5800, loss: 0.03011486306786537\n",
      "iter 5900, loss: 0.03078228235244751\n",
      "iter 6000, loss: 0.04259542375802994\n",
      "iter 6100, loss: 0.005927512422204018\n",
      "iter 6200, loss: 0.07643677294254303\n",
      "iter 6300, loss: 0.02907409705221653\n",
      "iter 6400, loss: 0.07557462155818939\n",
      "iter 6500, loss: 0.0062365904450416565\n",
      "iter 6600, loss: 0.07787887006998062\n",
      "iter 6700, loss: 0.027079498395323753\n",
      "iter 6800, loss: 0.05794844031333923\n",
      "iter 6900, loss: 0.009206652641296387\n",
      "iter 7000, loss: 0.01935521699488163\n",
      "iter 7100, loss: 0.03717587888240814\n",
      "iter 7200, loss: 0.03382321819663048\n",
      "iter 7300, loss: 0.03631395101547241\n",
      "iter 7400, loss: 0.03928663581609726\n",
      "iter 7500, loss: 0.12227292358875275\n",
      "iter 7600, loss: 0.04739625006914139\n",
      "iter 7700, loss: 0.10807124525308609\n",
      "iter 7800, loss: 0.03997011482715607\n",
      "iter 7900, loss: 0.003216065000742674\n",
      "iter 8000, loss: 0.03480144590139389\n",
      "iter 8100, loss: 0.008493364788591862\n",
      "iter 8200, loss: 0.08421193063259125\n",
      "iter 8300, loss: 0.04683050885796547\n",
      "iter 8400, loss: 0.017171679064631462\n",
      "iter 8500, loss: 0.10949604213237762\n",
      "iter 8600, loss: 0.009440628811717033\n",
      "iter 8700, loss: 0.03518041968345642\n",
      "iter 8800, loss: 0.11378499120473862\n",
      "iter 8900, loss: 0.05915773659944534\n",
      "iter 9000, loss: 0.036196090281009674\n",
      "iter 9100, loss: 0.020365502685308456\n",
      "iter 9200, loss: 0.005783037282526493\n",
      "iter 9300, loss: 0.013522477820515633\n",
      "iter 9400, loss: 0.012319231405854225\n",
      "iter 9500, loss: 0.039419181644916534\n",
      "iter 9600, loss: 0.010953864082694054\n",
      "iter 9700, loss: 0.02231159433722496\n",
      "iter 9800, loss: 0.022096233442425728\n",
      "iter 9900, loss: 0.03751145303249359\n",
      "iter 10000, loss: 0.01796252653002739\n",
      "iter 10100, loss: 0.05347774550318718\n",
      "iter 10200, loss: 0.033454202115535736\n",
      "iter 10300, loss: 0.005841377191245556\n",
      "iter 10400, loss: 0.0021574043203145266\n",
      "iter 10500, loss: 0.010327906347811222\n",
      "iter 10600, loss: 0.004629588685929775\n",
      "iter 10700, loss: 0.04275583475828171\n",
      "iter 10800, loss: 0.030024101957678795\n",
      "iter 10900, loss: 0.012727878987789154\n",
      "iter 11000, loss: 0.0039042197167873383\n",
      "iter 11100, loss: 0.05023513361811638\n",
      "iter 11200, loss: 0.06267930567264557\n",
      "iter 11300, loss: 0.05444765090942383\n",
      "iter 11400, loss: 0.01369427889585495\n",
      "iter 11500, loss: 0.05297108367085457\n",
      "iter 11600, loss: 0.011684229597449303\n",
      "iter 11700, loss: 0.004144283942878246\n",
      "iter 11800, loss: 0.007284649647772312\n",
      "iter 11900, loss: 0.006089288275688887\n",
      "iter 12000, loss: 0.0386037714779377\n",
      "iter 12100, loss: 0.001481592538766563\n",
      "iter 12200, loss: 0.004144513048231602\n",
      "iter 12300, loss: 0.015357020311057568\n",
      "iter 12400, loss: 0.180874302983284\n",
      "iter 12500, loss: 0.002411148976534605\n",
      "iter 12600, loss: 0.006420586258172989\n",
      "iter 12700, loss: 0.030053818598389626\n",
      "iter 12800, loss: 0.03314148262143135\n",
      "iter 12900, loss: 0.013353024609386921\n",
      "iter 13000, loss: 0.08192164450883865\n",
      "iter 13100, loss: 0.11021503061056137\n",
      "iter 13200, loss: 0.0148635758087039\n",
      "iter 13300, loss: 0.003942494746297598\n",
      "iter 13400, loss: 0.022511733695864677\n",
      "iter 13500, loss: 0.010573359206318855\n",
      "iter 13600, loss: 0.001926917931996286\n",
      "iter 13700, loss: 0.019295793026685715\n",
      "iter 13800, loss: 0.004394087009131908\n",
      "iter 13900, loss: 0.015224061906337738\n",
      "iter 14000, loss: 0.025674033910036087\n",
      "iter 14100, loss: 0.010257186368107796\n",
      "iter 14200, loss: 0.08001574128866196\n",
      "iter 14300, loss: 0.0184677354991436\n",
      "iter 14400, loss: 0.003682553768157959\n",
      "iter 14500, loss: 0.006013895384967327\n",
      "iter 14600, loss: 0.005410046316683292\n",
      "iter 14700, loss: 0.0815763846039772\n",
      "iter 14800, loss: 0.04575280100107193\n",
      "iter 14900, loss: 0.0034637190401554108\n",
      "iter 15000, loss: 0.2308529168367386\n",
      "iter 15100, loss: 0.0015718074282631278\n",
      "iter 15200, loss: 0.002342895371839404\n",
      "iter 15300, loss: 0.003715521190315485\n",
      "iter 15400, loss: 0.0275738425552845\n",
      "iter 15500, loss: 0.0022454685531556606\n",
      "iter 15600, loss: 0.007690155413001776\n",
      "iter 15700, loss: 0.022138219326734543\n",
      "iter 15800, loss: 0.012612427584826946\n",
      "iter 15900, loss: 0.08295592665672302\n",
      "iter 16000, loss: 0.013954073190689087\n",
      "iter 16100, loss: 0.0017462685937061906\n",
      "iter 16200, loss: 0.0035412597935646772\n",
      "iter 16300, loss: 0.006947445683181286\n",
      "iter 16400, loss: 0.003893845481798053\n",
      "iter 16500, loss: 0.0050964318215847015\n",
      "iter 16600, loss: 0.00756747554987669\n",
      "iter 16700, loss: 0.0042304727248847485\n",
      "iter 16800, loss: 0.24831601977348328\n",
      "iter 16900, loss: 0.03313484415411949\n",
      "iter 17000, loss: 0.010822294279932976\n",
      "iter 17100, loss: 0.00687816645950079\n",
      "iter 17200, loss: 0.002354917349293828\n",
      "iter 17300, loss: 0.011039783246815205\n",
      "iter 17400, loss: 0.00982593186199665\n",
      "iter 17500, loss: 0.16606935858726501\n",
      "iter 17600, loss: 0.0008666150388307869\n",
      "iter 17700, loss: 0.0014755085576325655\n",
      "iter 17800, loss: 0.02759731374680996\n",
      "iter 17900, loss: 0.017244094982743263\n",
      "iter 18000, loss: 0.009953027591109276\n",
      "iter 18100, loss: 0.004205845762044191\n",
      "iter 18200, loss: 0.0024765138514339924\n",
      "iter 18300, loss: 0.0023894228506833315\n",
      "iter 18400, loss: 0.01267248671501875\n",
      "iter 18500, loss: 0.00376588711515069\n",
      "iter 18600, loss: 0.0014296636218205094\n",
      "iter 18700, loss: 0.003545427229255438\n",
      "iter 18800, loss: 0.01612551510334015\n",
      "iter 18900, loss: 0.00693120900541544\n",
      "iter 19000, loss: 0.013783825561404228\n",
      "iter 19100, loss: 0.04698512703180313\n",
      "iter 19200, loss: 0.038609400391578674\n",
      "iter 19300, loss: 0.0012282583629712462\n",
      "iter 19400, loss: 0.00111121847294271\n",
      "iter 19500, loss: 0.002284898655489087\n",
      "iter 19600, loss: 0.014995652250945568\n",
      "iter 19700, loss: 0.01087932102382183\n",
      "iter 19800, loss: 0.004651003051549196\n",
      "iter 19900, loss: 0.013945460319519043\n",
      "iter 20000, loss: 0.009528153575956821\n",
      "iter 20100, loss: 0.004913046024739742\n",
      "iter 20200, loss: 0.0019566675182431936\n",
      "iter 20300, loss: 0.006808125413954258\n",
      "iter 20400, loss: 0.005869674030691385\n",
      "iter 20500, loss: 0.000945096486248076\n",
      "iter 20600, loss: 0.03143521025776863\n",
      "iter 20700, loss: 0.004107763059437275\n",
      "iter 20800, loss: 0.01274117548018694\n",
      "iter 20900, loss: 0.02667130157351494\n",
      "iter 21000, loss: 0.059666864573955536\n",
      "iter 21100, loss: 0.0005471902550198138\n",
      "iter 21200, loss: 0.011902106925845146\n",
      "iter 21300, loss: 0.001570573658682406\n",
      "iter 21400, loss: 0.007163016591221094\n",
      "iter 21500, loss: 0.031815044581890106\n",
      "iter 21600, loss: 0.004153884015977383\n",
      "iter 21700, loss: 0.002961159450933337\n",
      "iter 21800, loss: 0.006407085806131363\n",
      "iter 21900, loss: 0.07758239656686783\n",
      "iter 22000, loss: 0.0028625228442251682\n",
      "iter 22100, loss: 0.0018131860997527838\n",
      "iter 22200, loss: 0.0677543357014656\n",
      "iter 22300, loss: 0.001718592131510377\n",
      "iter 22400, loss: 0.0025717741809785366\n",
      "iter 22500, loss: 0.01909419521689415\n",
      "iter 22600, loss: 0.009273665025830269\n",
      "iter 22700, loss: 0.010256050154566765\n",
      "iter 22800, loss: 0.23724612593650818\n",
      "iter 22900, loss: 0.0021993988193571568\n",
      "iter 23000, loss: 0.0011628527427092195\n",
      "iter 23100, loss: 0.00561844976618886\n",
      "iter 23200, loss: 0.0011815171455964446\n",
      "iter 23300, loss: 0.00924534909427166\n",
      "iter 23400, loss: 0.016331292688846588\n",
      "iter 23500, loss: 0.0013368636136874557\n",
      "iter 23600, loss: 0.002119877841323614\n",
      "iter 23700, loss: 0.001068957382813096\n",
      "iter 23800, loss: 0.0005488589522428811\n",
      "iter 23900, loss: 0.0019007164519280195\n",
      "iter 24000, loss: 0.009306764230132103\n",
      "iter 24100, loss: 0.032638903707265854\n",
      "iter 24200, loss: 0.021500347182154655\n",
      "iter 24300, loss: 0.008478366769850254\n",
      "iter 24400, loss: 0.001394122838973999\n",
      "iter 24500, loss: 0.015977226197719574\n",
      "iter 24600, loss: 0.0011137162800878286\n",
      "iter 24700, loss: 0.0014894437044858932\n",
      "iter 24800, loss: 0.0028032150585204363\n",
      "iter 24900, loss: 0.26567360758781433\n",
      "iter 25000, loss: 0.005082236602902412\n",
      "iter 25100, loss: 0.004484409932047129\n",
      "iter 25200, loss: 0.0012839081464335322\n",
      "iter 25300, loss: 0.002063751919195056\n",
      "iter 25400, loss: 0.0008509645122103393\n",
      "iter 25500, loss: 0.016938211396336555\n",
      "iter 25600, loss: 0.0016137019265443087\n",
      "iter 25700, loss: 0.006316895596683025\n",
      "iter 25800, loss: 0.12360324710607529\n",
      "iter 25900, loss: 0.0009690852020867169\n",
      "iter 26000, loss: 0.0026425267569720745\n",
      "iter 26100, loss: 0.0023615879472345114\n",
      "iter 26200, loss: 0.003025151789188385\n",
      "iter 26300, loss: 0.004041597712785006\n",
      "iter 26400, loss: 0.0010049500269815326\n",
      "iter 26500, loss: 0.0044799912720918655\n",
      "iter 26600, loss: 0.0002939734549727291\n",
      "iter 26700, loss: 0.005397153086960316\n",
      "iter 26800, loss: 0.001910471124574542\n",
      "iter 26900, loss: 0.007450486067682505\n",
      "iter 27000, loss: 0.01450827531516552\n",
      "iter 27100, loss: 0.009971429593861103\n",
      "iter 27200, loss: 0.013369044288992882\n",
      "iter 27300, loss: 0.1290353685617447\n",
      "iter 27400, loss: 0.004283726215362549\n",
      "iter 27500, loss: 0.042634207755327225\n",
      "iter 27600, loss: 0.006020468659698963\n",
      "iter 27700, loss: 0.02201438695192337\n",
      "iter 27800, loss: 0.0020105831790715456\n",
      "iter 27900, loss: 0.051119714975357056\n",
      "iter 28000, loss: 0.00011200492735952139\n",
      "iter 28100, loss: 0.0008167921332642436\n",
      "iter 28200, loss: 0.08281658589839935\n",
      "iter 28300, loss: 0.07781030237674713\n",
      "iter 28400, loss: 0.02727302350103855\n",
      "iter 28500, loss: 0.0012216500472277403\n",
      "iter 28600, loss: 0.04096986725926399\n",
      "iter 28700, loss: 0.006930649280548096\n",
      "iter 28800, loss: 0.0029450240544974804\n",
      "iter 28900, loss: 0.0026135556399822235\n",
      "iter 29000, loss: 0.0015738267684355378\n",
      "iter 29100, loss: 0.001281294971704483\n",
      "iter 29200, loss: 0.00345818093046546\n",
      "iter 29300, loss: 0.011684758588671684\n",
      "iter 29400, loss: 0.0015556926373392344\n",
      "iter 29500, loss: 0.0016010419931262732\n",
      "iter 29600, loss: 0.00040377723053097725\n",
      "iter 29700, loss: 0.0023737538140267134\n",
      "iter 29800, loss: 0.004503943026065826\n",
      "iter 29900, loss: 0.004385339096188545\n",
      "iter 30000, loss: 0.04661201685667038\n",
      "iter 30100, loss: 0.06277456134557724\n",
      "iter 30200, loss: 0.0021947091445326805\n",
      "iter 30300, loss: 0.0021564855705946684\n",
      "iter 30400, loss: 0.011491210199892521\n",
      "iter 30500, loss: 0.006159500684589148\n",
      "iter 30600, loss: 0.019063282757997513\n",
      "iter 30700, loss: 0.0034937155432999134\n",
      "iter 30800, loss: 0.011671876534819603\n",
      "iter 30900, loss: 0.00424572080373764\n",
      "iter 31000, loss: 0.0006091550458222628\n",
      "iter 31100, loss: 0.04517421871423721\n",
      "iter 31200, loss: 0.0014559559058398008\n",
      "iter 31300, loss: 0.026261311024427414\n",
      "iter 31400, loss: 0.0038856796454638243\n",
      "iter 31500, loss: 0.0015611067647114396\n",
      "iter 31600, loss: 0.0425361767411232\n",
      "iter 31700, loss: 0.14619660377502441\n",
      "iter 31800, loss: 0.019569233059883118\n",
      "iter 31900, loss: 0.004355308599770069\n",
      "iter 32000, loss: 0.008778022602200508\n",
      "iter 32100, loss: 0.0007183713605627418\n",
      "iter 32200, loss: 0.009436553344130516\n",
      "iter 32300, loss: 0.000803756236564368\n",
      "iter 32400, loss: 0.0052415356040000916\n",
      "iter 32500, loss: 0.1325892210006714\n",
      "iter 32600, loss: 0.0009127741795964539\n",
      "iter 32700, loss: 0.0009948961669579148\n",
      "iter 32800, loss: 0.0013393404660746455\n",
      "iter 32900, loss: 0.000512671482283622\n",
      "iter 33000, loss: 0.0004619185929186642\n",
      "iter 33100, loss: 0.040784578770399094\n",
      "iter 33200, loss: 0.0035237285774201155\n",
      "iter 33300, loss: 0.008143803104758263\n",
      "iter 33400, loss: 0.010415853932499886\n",
      "iter 33500, loss: 0.007088333368301392\n",
      "iter 33600, loss: 0.0009460495784878731\n",
      "iter 33700, loss: 0.0011818459024652839\n",
      "iter 33800, loss: 0.001506535685621202\n",
      "iter 33900, loss: 0.003035311121493578\n",
      "iter 34000, loss: 0.02066102810204029\n",
      "iter 34100, loss: 0.052974242717027664\n",
      "iter 34200, loss: 0.0009492298704572022\n",
      "iter 34300, loss: 0.0006232126033864915\n",
      "iter 34400, loss: 0.0046613202430307865\n",
      "iter 34500, loss: 0.000986170838586986\n",
      "iter 34600, loss: 0.004776028450578451\n",
      "iter 34700, loss: 0.0036792848259210587\n",
      "iter 34800, loss: 0.0012092893011868\n",
      "iter 34900, loss: 0.07772768288850784\n",
      "iter 35000, loss: 0.0006671876180917025\n",
      "iter 35100, loss: 0.001024141674861312\n",
      "iter 35200, loss: 0.004872161895036697\n",
      "iter 35300, loss: 0.0038381200283765793\n",
      "iter 35400, loss: 0.005940540228039026\n",
      "iter 35500, loss: 0.02484264224767685\n",
      "iter 35600, loss: 0.0007805947680026293\n",
      "iter 35700, loss: 0.0005075239459984004\n",
      "iter 35800, loss: 0.0014438159996643662\n",
      "iter 35900, loss: 0.0019926608074456453\n",
      "iter 36000, loss: 0.013628748245537281\n",
      "iter 36100, loss: 0.025078022852540016\n",
      "iter 36200, loss: 0.0005279830074869096\n",
      "iter 36300, loss: 0.0022519398480653763\n",
      "iter 36400, loss: 0.000739879033062607\n",
      "iter 36500, loss: 0.0011333078145980835\n",
      "iter 36600, loss: 0.1473737210035324\n",
      "iter 36700, loss: 0.005806895904242992\n",
      "iter 36800, loss: 0.000698608229868114\n",
      "iter 36900, loss: 0.0010149511508643627\n",
      "iter 37000, loss: 0.005930522922426462\n",
      "iter 37100, loss: 0.006768459919840097\n",
      "iter 37200, loss: 0.006438361946493387\n",
      "iter 37300, loss: 0.0029705450870096684\n",
      "iter 37400, loss: 0.0017306699883192778\n",
      "iter 37500, loss: 0.002768476726487279\n",
      "iter 37600, loss: 0.006143811624497175\n",
      "iter 37700, loss: 0.001869270228780806\n",
      "iter 37800, loss: 0.0007571355672553182\n",
      "iter 37900, loss: 0.0007730296929366887\n",
      "iter 38000, loss: 0.003915796056389809\n",
      "iter 38100, loss: 0.000826483010314405\n",
      "iter 38200, loss: 0.00040819417336024344\n",
      "iter 38300, loss: 0.00018723115499597043\n",
      "iter 38400, loss: 0.018058575689792633\n",
      "iter 38500, loss: 0.001310445717535913\n",
      "iter 38600, loss: 0.001156763406470418\n",
      "iter 38700, loss: 0.006937210913747549\n",
      "iter 38800, loss: 0.0005773332086391747\n",
      "iter 38900, loss: 0.0032446507830172777\n",
      "iter 39000, loss: 0.0041894325986504555\n",
      "iter 39100, loss: 0.002270395401865244\n",
      "iter 39200, loss: 0.0016876973677426577\n",
      "iter 39300, loss: 0.09975703060626984\n",
      "iter 39400, loss: 0.0022354188840836287\n",
      "iter 39500, loss: 0.042563170194625854\n",
      "iter 39600, loss: 0.0025960104539990425\n",
      "iter 39700, loss: 0.0018203677609562874\n",
      "iter 39800, loss: 0.007005223538726568\n",
      "iter 39900, loss: 0.028228336945176125\n",
      "iter 40000, loss: 0.0646870881319046\n",
      "iter 40100, loss: 0.0016307998448610306\n",
      "iter 40200, loss: 0.0008898103842511773\n",
      "iter 40300, loss: 0.0010684042936190963\n",
      "iter 40400, loss: 0.00020840174693148583\n",
      "iter 40500, loss: 0.003480025799944997\n",
      "iter 40600, loss: 0.03110269270837307\n",
      "iter 40700, loss: 0.0012970308307558298\n",
      "iter 40800, loss: 0.00139926141127944\n",
      "iter 40900, loss: 0.0024722483940422535\n",
      "iter 41000, loss: 0.0002732003922574222\n",
      "iter 41100, loss: 0.00034838420106098056\n",
      "iter 41200, loss: 0.06314001977443695\n",
      "iter 41300, loss: 0.12470356374979019\n",
      "iter 41400, loss: 0.08653012663125992\n",
      "iter 41500, loss: 0.31657737493515015\n",
      "iter 41600, loss: 0.015275241807103157\n",
      "iter 41700, loss: 0.02794361673295498\n",
      "iter 41800, loss: 0.0007841165061108768\n",
      "iter 41900, loss: 0.02042410522699356\n",
      "iter 42000, loss: 0.16457249224185944\n",
      "iter 42100, loss: 0.2482248991727829\n",
      "iter 42200, loss: 0.4412460923194885\n",
      "iter 42300, loss: 0.0001438962935935706\n",
      "iter 42400, loss: 0.00033073307713493705\n",
      "iter 42500, loss: 0.0013674285728484392\n",
      "iter 42600, loss: 0.0017211741069331765\n",
      "iter 42700, loss: 0.00154106505215168\n",
      "iter 42800, loss: 0.011157454922795296\n",
      "iter 42900, loss: 0.003320036455988884\n",
      "iter 43000, loss: 0.0006219345377758145\n",
      "iter 43100, loss: 0.001880368567071855\n",
      "iter 43200, loss: 0.0009320267708972096\n",
      "iter 43300, loss: 0.001133579295128584\n",
      "iter 43400, loss: 0.0003611353749874979\n",
      "iter 43500, loss: 0.004694900009781122\n",
      "iter 43600, loss: 0.09268642961978912\n",
      "iter 43700, loss: 0.0012925645569339395\n",
      "iter 43800, loss: 0.0026342577766627073\n",
      "iter 43900, loss: 0.001073508057743311\n",
      "iter 44000, loss: 0.0018630096456035972\n",
      "iter 44100, loss: 0.09038929641246796\n",
      "iter 44200, loss: 0.028116125613451004\n",
      "iter 44300, loss: 0.003423467744141817\n",
      "iter 44400, loss: 0.0006371718482114375\n",
      "iter 44500, loss: 0.0009707695571705699\n",
      "iter 44600, loss: 0.009720196016132832\n",
      "iter 44700, loss: 0.0020022480748593807\n",
      "iter 44800, loss: 0.028025200590491295\n",
      "iter 44900, loss: 0.0002848994918167591\n",
      "iter 45000, loss: 0.0004605863359756768\n",
      "iter 45100, loss: 0.31455159187316895\n",
      "iter 45200, loss: 0.001962067326530814\n",
      "iter 45300, loss: 0.00010031579586211592\n",
      "iter 45400, loss: 0.003110721241682768\n",
      "iter 45500, loss: 0.04116173833608627\n",
      "iter 45600, loss: 0.0013175446074455976\n",
      "iter 45700, loss: 0.0004806391953025013\n",
      "iter 45800, loss: 0.000728696002624929\n",
      "iter 45900, loss: 0.012097996659576893\n",
      "iter 46000, loss: 0.0005074903601780534\n",
      "iter 46100, loss: 0.003369869664311409\n",
      "iter 46200, loss: 0.0010897881584241986\n",
      "iter 46300, loss: 0.006739961914718151\n",
      "iter 46400, loss: 0.0008392368326894939\n",
      "iter 46500, loss: 0.0006667920970357955\n",
      "iter 46600, loss: 0.0013921487843617797\n",
      "iter 46700, loss: 0.0014527849853038788\n",
      "iter 46800, loss: 0.00021903915330767632\n",
      "iter 46900, loss: 0.007871129550039768\n",
      "iter 47000, loss: 0.0030279068741947412\n",
      "iter 47100, loss: 0.00034050754038617015\n",
      "iter 47200, loss: 0.001314358552917838\n",
      "iter 47300, loss: 8.118651749100536e-05\n",
      "iter 47400, loss: 0.006149906665086746\n",
      "iter 47500, loss: 0.018466172739863396\n",
      "iter 47600, loss: 0.04907948896288872\n",
      "iter 47700, loss: 0.008582718670368195\n",
      "iter 47800, loss: 0.0004186645965091884\n",
      "iter 47900, loss: 0.012796345166862011\n",
      "iter 48000, loss: 0.0008666574722155929\n",
      "iter 48100, loss: 0.0006312112091109157\n",
      "iter 48200, loss: 0.00013576784112956375\n",
      "iter 48300, loss: 0.00018279529467690736\n",
      "iter 48400, loss: 0.0002575781545601785\n",
      "iter 48500, loss: 0.009594801813364029\n",
      "iter 48600, loss: 0.009564513340592384\n",
      "iter 48700, loss: 0.012244273908436298\n",
      "iter 48800, loss: 0.005253498442471027\n",
      "iter 48900, loss: 0.21926024556159973\n",
      "iter 49000, loss: 0.007294267416000366\n",
      "iter 49100, loss: 0.0032061103265732527\n",
      "iter 49200, loss: 0.01819906011223793\n",
      "iter 49300, loss: 0.004881418775767088\n",
      "iter 49400, loss: 0.0008549763006158173\n",
      "iter 49500, loss: 0.026015019044280052\n",
      "iter 49600, loss: 0.0879039540886879\n",
      "iter 49700, loss: 0.002169793238863349\n",
      "iter 49800, loss: 0.0028638504445552826\n",
      "iter 49900, loss: 0.00010671402560546994\n",
      "iter 50000, loss: 0.002431075554341078\n",
      "iter 50100, loss: 0.08110547065734863\n",
      "iter 50200, loss: 0.0011746416566893458\n",
      "iter 50300, loss: 0.004063538275659084\n",
      "iter 50400, loss: 0.003638938069343567\n",
      "iter 50500, loss: 0.02355579100549221\n",
      "iter 50600, loss: 0.00020870997104793787\n",
      "iter 50700, loss: 0.0007861051708459854\n",
      "iter 50800, loss: 0.0011907799635082483\n",
      "iter 50900, loss: 0.007494129240512848\n",
      "iter 51000, loss: 0.0037664882838726044\n",
      "iter 51100, loss: 0.0013115499168634415\n",
      "iter 51200, loss: 0.000791001773905009\n",
      "iter 51300, loss: 0.0005909923929721117\n",
      "iter 51400, loss: 0.0012468002969399095\n",
      "iter 51500, loss: 0.00045511615462601185\n",
      "iter 51600, loss: 0.0008897886727936566\n",
      "iter 51700, loss: 0.002589704003185034\n",
      "iter 51800, loss: 0.016058914363384247\n",
      "iter 51900, loss: 0.0006226809346117079\n",
      "iter 52000, loss: 0.003497319296002388\n",
      "iter 52100, loss: 0.14527329802513123\n",
      "iter 52200, loss: 0.005687866825610399\n",
      "iter 52300, loss: 0.0012183650396764278\n",
      "iter 52400, loss: 0.046947579830884933\n",
      "iter 52500, loss: 0.0013420956674963236\n",
      "iter 52600, loss: 0.00023533779312856495\n",
      "iter 52700, loss: 0.02724352851510048\n",
      "iter 52800, loss: 0.0005074063083156943\n",
      "iter 52900, loss: 0.2169663906097412\n",
      "iter 53000, loss: 0.000878175429534167\n",
      "iter 53100, loss: 0.016868727281689644\n",
      "iter 53200, loss: 0.0014800397912040353\n",
      "iter 53300, loss: 0.00027854222571477294\n",
      "iter 53400, loss: 0.07587923109531403\n",
      "iter 53500, loss: 0.005116692744195461\n",
      "iter 53600, loss: 0.004881688859313726\n",
      "iter 53700, loss: 0.0004170493339188397\n",
      "iter 53800, loss: 7.572702452307567e-05\n",
      "iter 53900, loss: 0.0002475680666975677\n",
      "iter 54000, loss: 0.4170856475830078\n",
      "iter 54100, loss: 0.006746791303157806\n",
      "iter 54200, loss: 0.0025407173670828342\n",
      "iter 54300, loss: 0.00041539641097187996\n",
      "iter 54400, loss: 0.0010729475179687142\n",
      "iter 54500, loss: 0.002018281724303961\n",
      "iter 54600, loss: 0.001125825336202979\n",
      "iter 54700, loss: 0.0011397332418709993\n",
      "iter 54800, loss: 0.0007603760459460318\n",
      "iter 54900, loss: 0.0019176850328221917\n",
      "iter 55000, loss: 0.0034469780512154102\n",
      "iter 55100, loss: 0.004003929905593395\n",
      "iter 55200, loss: 0.007647173013538122\n",
      "iter 55300, loss: 0.0005364055978134274\n",
      "iter 55400, loss: 0.00355294207111001\n",
      "iter 55500, loss: 0.000727999722585082\n",
      "iter 55600, loss: 0.005140350200235844\n",
      "iter 55700, loss: 0.00039626401849091053\n",
      "iter 55800, loss: 0.0005243924679234624\n",
      "iter 55900, loss: 0.015790529549121857\n",
      "iter 56000, loss: 0.0002215205749962479\n",
      "iter 56100, loss: 0.001195845426991582\n",
      "iter 56200, loss: 0.019381359219551086\n",
      "iter 56300, loss: 0.00031687080627307296\n",
      "iter 56400, loss: 0.008609719574451447\n",
      "iter 56500, loss: 0.007167898118495941\n",
      "iter 56600, loss: 0.002987453481182456\n",
      "iter 56700, loss: 0.00095791881904006\n",
      "iter 56800, loss: 0.0001140948006650433\n",
      "iter 56900, loss: 0.001249184599146247\n",
      "iter 57000, loss: 0.002941705286502838\n",
      "iter 57100, loss: 0.0074362969025969505\n",
      "iter 57200, loss: 0.004251205362379551\n",
      "iter 57300, loss: 0.0026287001091986895\n",
      "iter 57400, loss: 0.0011364074889570475\n",
      "iter 57500, loss: 0.0011320664780214429\n",
      "iter 57600, loss: 0.004145718179643154\n",
      "iter 57700, loss: 0.0018101843306794763\n",
      "iter 57800, loss: 0.00021686291438527405\n",
      "iter 57900, loss: 0.0011854617623612285\n",
      "iter 58000, loss: 0.0001907518890220672\n",
      "iter 58100, loss: 0.001780705526471138\n",
      "iter 58200, loss: 0.004325260408222675\n",
      "iter 58300, loss: 0.0030507780611515045\n",
      "iter 58400, loss: 0.00030592954135499895\n",
      "iter 58500, loss: 0.0007768088835291564\n",
      "iter 58600, loss: 0.004402820952236652\n",
      "iter 58700, loss: 0.0007284063613042235\n",
      "iter 58800, loss: 0.027379268780350685\n",
      "iter 58900, loss: 0.0035883337259292603\n",
      "iter 59000, loss: 0.00012985835201106966\n",
      "iter 59100, loss: 0.0004241115821059793\n",
      "iter 59200, loss: 0.0012948501389473677\n",
      "iter 59300, loss: 0.0018942259484902024\n",
      "iter 59400, loss: 0.019991498440504074\n",
      "iter 59500, loss: 0.00032267457572743297\n",
      "iter 59600, loss: 0.0003421608416829258\n",
      "iter 59700, loss: 5.239136589807458e-05\n",
      "iter 59800, loss: 0.0011588388588279486\n",
      "iter 59900, loss: 0.05537568777799606\n",
      "iter 60000, loss: 0.017172779887914658\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(mnistdataset,1)\n",
    "\n",
    "d = D()\n",
    "\n",
    "for data,target in dataloader:\n",
    "    d.train(data.view(-1,1,28,28),target.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDatasetTest(Dataset):\n",
    "    def __init__(self,file_path):\n",
    "        super().__init__()\n",
    "        self.dataset = pd.read_csv(file_path, header=None)\n",
    "        self.array = self.dataset.to_numpy()\n",
    "\n",
    "        # self.dataset -> numpy\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.array)\n",
    "\n",
    "    def __getitem__(self, index) :\n",
    "        target = self.array[index,0]\n",
    "        boolean_indexing = np.zeros(10)\n",
    "        boolean_indexing[target] = 1.0\n",
    "        return self.array[index,0],torch.FloatTensor(self.array[index,1:].reshape(28,28)/255.0),torch.FloatTensor(boolean_indexing)\n",
    "    def plot_img(self,index):\n",
    "        data = self.array[index,1:].reshape(28,28)\n",
    "        plt.figure(figsize=(3,3))\n",
    "        plt.imshow(data,cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_dataset = MnistDatasetTest('mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9721 10000 0.9721\n"
     ]
    }
   ],
   "source": [
    "score = 0\n",
    "items = 0\n",
    "\n",
    "for label, image_data_tensor, target_tensor in mnist_test_dataset:\n",
    "    answer = d.forward(image_data_tensor.view(1,1,28,28)).detach().numpy()\n",
    "    if (answer.argmax() == label):\n",
    "        score += 1\n",
    "        pass\n",
    "    items += 1\n",
    "    \n",
    "    pass\n",
    "\n",
    "print(score, items, score/items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "529077712abe41158b2333534864632ac4fef210637a7213b73e6e4fe2bb3f3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
